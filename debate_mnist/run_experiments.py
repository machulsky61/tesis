import subprocess
import sys
import os
import json
from datetime import datetime
from collections import defaultdict
from pathlib import Path
from utils.paths import get_config_path

class ExperimentManager:
    def __init__(self):
        self.experiments = []
        self.available_judges = []
        # Global configuration - set once and reused
        self.global_config = {
            'judge_name': "28",  # default judge
            'resolution': 28,  # default resolution
            'k': 6,  # default pixels
            'thr': 0.0,
            'default_images_greedy': 1000,
            'default_images_mcts': 100,  # updated value
            'default_rollouts': 512,  # updated value
            'seed': 42,
            'allow_all_pixels': False,
            'track_confidence': False
        }
        # Experiment-specific configuration
        self.config = {
            'viz_enabled': False,
            'viz_colored': False,
            'viz_metadata': False,
            'save_images': False,
            'save_masks': False,
            'save_play': False,
            'save_confidence': False,
            'save_json': False,  # New comprehensive JSON format
            'track_logits': False,
            'track_logits_progressive': False,
            'save_raw_logits': False
        }
        
    def print_banner(self):
        print("=" * 80)
        print()
        print("ğŸ§ª AI SAFETY DEBATE - EXPERIMENT AUTOMATION v3.5 ğŸ§ª")


    def get_input(self, prompt, default=None, options=None, input_type=str):
        """Gets user input with validation."""
        while True:
            if default is not None:
                user_input = input(f"{prompt} [{default}]: ").strip()
                if not user_input:
                    return default
            else:
                user_input = input(f"{prompt}: ").strip()
            
            if options and user_input.lower() not in [opt.lower() for opt in options]:
                print(f"âŒ Invalid option. Options: {', '.join(options)}")
                continue
            
            try:
                if input_type == int:
                    return int(user_input)
                elif input_type == float:
                    return float(user_input)
                return user_input
            except ValueError:
                print(f"âŒ Please enter a valid {input_type.__name__}")

    def get_yes_no(self, prompt, default="n"):
        """Gets yes/no response from user."""
        response = self.get_input(f"{prompt} (y/n)", default, ["y", "n", "yes", "no"])
        return response.lower() in ["y", "yes"]

    def check_judge_exists(self, judge_name):
        """Checks if a judge model exists."""
        return os.path.exists(f"models/{judge_name}.pth")

    def detect_judge_params(self, judge_name):
        """Detecta parÃ¡metros del juez basÃ¡ndose en el nombre."""
        # ResoluciÃ³n
        if "16" in judge_name:
            resolution = 16
        else:
            resolution = 28
        
        # PÃ­xeles (k)
        if "_4px" in judge_name:
            k = 4
        elif "_3px" in judge_name:
            k = 3
        elif "_5px" in judge_name:
            k = 5
        elif "_8px" in judge_name:
            k = 8
        elif "_10px" in judge_name:
            k = 10
        else:
            k = 6  # default
            
        return resolution, k

    def scan_available_judges(self):
        """Escanea todos los jueces disponibles en models/"""
        self.available_judges = []
        if os.path.exists("models"):
            for file in os.listdir("models"):
                if file.endswith(".pth"):
                    judge_name = file[:-4]  # remover .pth
                    self.available_judges.append(judge_name)
        
        # Ordenar: primero los estÃ¡ndar, luego los custom
        standard = ["28", "16", "28_4px", "16_4px"]
        self.available_judges.sort(key=lambda x: (x not in standard, x))

    def ask_override_global(self, param_name, current_value, description):
        """Pregunta si el usuario quiere cambiar un parÃ¡metro global."""
        if current_value is not None:
            return self.get_yes_no(f"Â¿Cambiar {description} (actual: {current_value})?")
        return True

    def show_main_menu(self):
        """Muestra el menÃº principal."""
        while True:
            print("\n" + "="*80)
            print("ğŸ“‹ MENÃš PRINCIPAL")
            print("="*80)
            print("1. ğŸ“ Entrenar nuevos modelos juez")
            print("2. ğŸ”¬ Configurar y ejecutar experimentos")
            print("3. ğŸ¯ Evaluar capacidades del juez")
            print("4. ğŸ“Š AnÃ¡lisis de tesis y visualizaciones")
            print("5. ğŸ“ˆ Ver resultados anteriores")
            print("6. ğŸ’¾ Cargar configuraciÃ³n guardada")
            print("7. ğŸ“ Crear template de experimentos")
            print("8. âŒ Salir")
            
            choice = self.get_input("Selecciona una opciÃ³n", "2", ["1", "2", "3", "4", "5", "6", "7", "8"])
            
            if choice == "1":
                self.train_judges()
            elif choice == "2":
                self.configure_experiments()
            elif choice == "3":
                self.evaluate_judge()
            elif choice == "4":
                self.thesis_analysis()
            elif choice == "5":
                self.show_previous_results()
            elif choice == "6":
                self.load_configuration()
            elif choice == "7":
                self.create_template()
            elif choice == "8":
                print("ğŸ‘‹ Â¡Hasta luego!")
                sys.exit(0)

    def thesis_analysis(self):
        """Sistema completo de anÃ¡lisis de tesis."""
        print("\n" + "="*80)
        print("ğŸ“Š ANÃLISIS DE TESIS Y VISUALIZACIONES")
        print("="*80)
        print("Complete system for generating debate experiment data.")
        print("\nCaracterÃ­sticas:")
        print("â€¢ CSV data export for statistical analysis")
        print("â€¢ AnÃ¡lisis estadÃ­stico con intervalos de confianza")
        print("â€¢ Tablas LaTeX para la tesis")
        print("â€¢ Notebook interactivo Jupyter")
        print("â€¢ ValidaciÃ³n de consistencia de datos")
        
        # Check for available data files
        required_files = ["outputs/debates.csv", "outputs/evaluations.csv"]
        missing_files = [f for f in required_files if not os.path.exists(f)]
        if missing_files:
            print("âš ï¸ Archivos de datos faltantes:")
            for f in missing_files:
                print(f"  â€¢ {f}")
            print("\nEjecuta experimentos primero para generar datos.")
            if not self.get_yes_no("Â¿Continuar de todas formas?"):
                return
        
        while True:
            print("\n" + "="*60)
            print("ğŸ“‹ OPCIONES DE ANÃLISIS")
            print("="*60)
            print("1. ğŸ“ˆ Generate analysis data (CSV export)")
            print("2. ğŸ” AnÃ¡lisis de datos interactivo")
            print("3. ğŸ“Š Resumen estadÃ­stico completo")
            print("4. ğŸ“‹ ValidaciÃ³n de consistencia de datos")
            print("5. ğŸ“ˆ Custom data analysis")
            print("6. ğŸ“ Generar tablas LaTeX")
            print("7. ğŸ““ Abrir Jupyter notebook interactivo")
            print("8. ğŸš€ Complete data analysis workflow")
            print("0. â†©ï¸ Volver al menÃº principal")
            
            choice = self.get_input("Selecciona opciÃ³n", "1", ["0", "1", "2", "3", "4", "5", "6", "7", "8"])
            
            if choice == "0":
                break
            elif choice == "1":
                self._generate_all_thesis_figures()
            elif choice == "8":
                self._complete_thesis_analysis()

    def _generate_all_thesis_figures(self):
        """Ejecuta el mÃ³dulo private analysis para generar figuras de tesis."""
        print("\nğŸ“Š MÃ“DULO PRIVATE ANALYSIS - FIGURAS DE TESIS")
        print("=" * 60)
        print("ğŸ“ Ejecutando mÃ³dulo private analysis real")
        print("ğŸ¯ UbicaciÃ³n: .private_analysis/analysis/run_visualizations.py")
        
        try:
            # Change to the private analysis directory
            private_analysis_dir = Path(".private_analysis/analysis")
            original_dir = Path.cwd()
            
            if not private_analysis_dir.exists():
                print("âŒ MÃ³dulo private analysis no encontrado")
                print("ğŸ’¡ Esperado en: .private_analysis/analysis/")
                return
            
            print(f"\nğŸ”§ Cambiando al directorio: {private_analysis_dir.absolute()}")
            os.chdir(private_analysis_dir)
            
            # Execute the private analysis visualization manager
            print("ğŸš€ Ejecutando run_visualizations.py...")
            result = subprocess.run([
                sys.executable, "run_visualizations.py"
            ], capture_output=True, text=True, timeout=300)
            
            print("\nğŸ“‹ SALIDA DEL MÃ“DULO PRIVATE ANALYSIS:")
            print("-" * 50)
            if result.stdout:
                print(result.stdout)
            if result.stderr:
                print("âš ï¸ Advertencias/Errores:")
                print(result.stderr)
            
            if result.returncode == 0:
                print("\nâœ… MÃ“DULO PRIVATE ANALYSIS EJECUTADO EXITOSAMENTE")
                print("ğŸ“ Revisa: .private_analysis/analysis/output/")
            else:
                print(f"\nâŒ Error en mÃ³dulo private analysis (cÃ³digo: {result.returncode})")
                
        except subprocess.TimeoutExpired:
            print("â° Timeout del mÃ³dulo private analysis (5 minutos)")
        except Exception as e:
            print(f"âŒ Error ejecutando mÃ³dulo private analysis: {e}")
        finally:
            # Always return to original directory
            os.chdir(original_dir)
            print(f"ğŸ”™ Retornado a: {original_dir}")

    def _complete_thesis_analysis(self):
        """AnÃ¡lisis completo de tesis."""
        print("\nğŸš€ ANÃLISIS COMPLETO DE TESIS")
        print("=" * 50)
        print("Executing complete data analysis workflow")
        
        self._generate_all_thesis_figures()
        print("\nğŸ‰ ANÃLISIS COMPLETO FINALIZADO")
        print("ğŸ“‹ Archivos generados:")
        print("â€¢ analysis/output/ - Figuras PNG de alta calidad")
        print("â€¢ outputs/ - CSVs con datos de experimentos")
        print("â€¢ Salida de consola - EstadÃ­sticas y tablas LaTeX")

    def train_judges(self):
        """Maneja el entrenamiento de modelos juez."""
        print("\n" + "="*80)
        print("ğŸ“ ENTRENAMIENTO DE MODELOS JUEZ")
        print("="*80)
        print("Entrena modelos SparseCNN para diferentes configuraciones de resoluciÃ³n y k.")
        
        # Configuraciones estÃ¡ndar expandidas
        standard_configs = [
            ("28", 28, 6, "ResoluciÃ³n 28x28, 6 pÃ­xeles (PRINCIPAL)"),
            ("16", 16, 6, "ResoluciÃ³n 16x16, 6 pÃ­xeles"),
            ("28_4px", 28, 4, "ResoluciÃ³n 28x28, 4 pÃ­xeles"),
            ("16_4px", 16, 4, "ResoluciÃ³n 16x16, 4 pÃ­xeles"),
            ("28_3px", 28, 3, "ResoluciÃ³n 28x28, 3 pÃ­xeles"),
            ("28_5px", 28, 5, "ResoluciÃ³n 28x28, 5 pÃ­xeles"),
            ("28_8px", 28, 8, "ResoluciÃ³n 28x28, 8 pÃ­xeles"),
        ]
        
        print("\nğŸ“Š Estado de modelos juez:")
        print("-" * 60)
        for i, (judge_name, resolution, k, description) in enumerate(standard_configs, 1):
            status = "âœ…" if self.check_judge_exists(judge_name) else "âŒ"
            print(f"{i:2d}. {status} {judge_name:<12} - {description}")
        
        print(f"\n{len(standard_configs)+1}. ğŸ”§ ConfiguraciÃ³n personalizada")
        print(f"{len(standard_configs)+2}. â†©ï¸  Volver al menÃº principal")
        
        # SelecciÃ³n mÃºltiple
        judges_to_train = []
        while True:
            choice = self.get_input("\nSelecciona opciÃ³n (o 'done' para terminar)", 
                                  str(len(standard_configs)+2))
            
            if choice.lower() == 'done' or choice == str(len(standard_configs)+2):
                break
                
            if choice == str(len(standard_configs)+1):
                # ConfiguraciÃ³n personalizada
                custom_name = self.get_input("Nombre del juez personalizado")
                custom_resolution = self.get_input("ResoluciÃ³n", "28", input_type=int)
                custom_k = self.get_input("NÃºmero de pÃ­xeles (k)", "6", input_type=int)
                custom_epochs = self.get_input("Ã‰pocas", "64", input_type=int)
                judges_to_train.append((custom_name, custom_resolution, custom_k, 
                                      custom_epochs, "Personalizado"))
            else:
                try:
                    idx = int(choice) - 1
                    if 0 <= idx < len(standard_configs):
                        judge_name, resolution, k, description = standard_configs[idx]
                        epochs = self.get_input(f"Ã‰pocas para {judge_name}", "64", input_type=int)
                        judges_to_train.append((judge_name, resolution, k, epochs, description))
                except:
                    print("âŒ OpciÃ³n invÃ¡lida")
        
        if judges_to_train:
            self._execute_training(judges_to_train)

    def _execute_training(self, judges_to_train):
        """Ejecuta el entrenamiento de los jueces seleccionados."""
        print(f"\nğŸ“‹ Se entrenarÃ¡n {len(judges_to_train)} modelos:")
        for judge_name, _, _, epochs, desc in judges_to_train:
            print(f"  â€¢ {judge_name}: {desc} ({epochs} Ã©pocas)")
        
        if self.get_yes_no("\nÂ¿Proceder con el entrenamiento?"):
            for judge_name, resolution, k, epochs, description in judges_to_train:
                cmd = f"python train_judge.py --judge_name {judge_name} --resolution {resolution} --k {k} --epochs {epochs}"
                if self.run_command(cmd, f"Entrenando {judge_name}"):
                    print(f"âœ… {judge_name} entrenado exitosamente")
                    self.available_judges.append(judge_name)
                else:
                    print(f"âŒ Error entrenando {judge_name}")
                    if not self.get_yes_no("Â¿Continuar con los demÃ¡s?"):
                        break

    def evaluate_judge(self):
        """EvalÃºa capacidades del juez con diferentes estrategias."""
        print("\n" + "="*80)
        print("ğŸ¯ EVALUACIÃ“N DE CAPACIDADES DEL JUEZ")
        print("="*80)
        print("EvalÃºa la precisiÃ³n del juez con 8 estrategias diferentes de selecciÃ³n de pÃ­xeles:")
        print("â€¢ Estrategias estÃ¡ticas: random, optimal, adversarial")
        print("â€¢ Agentes secuenciales: greedy, MCTS, adversariales")
        print("â€¢ AnÃ¡lisis comparativo y de escalabilidad")
        
        # Escanear jueces disponibles
        self.scan_available_judges()
        
        if not self.available_judges:
            print("âŒ No hay modelos juez disponibles. Entrena uno primero.")
            return
        
        # Seleccionar juez
        print("\nğŸ“Š Modelos juez disponibles:")
        for i, judge in enumerate(self.available_judges, 1):
            res, k = self.detect_judge_params(judge)
            print(f"{i:2d}. {judge} (res: {res}x{res}, k: {k})")
        
        idx = self.get_input("Selecciona juez (nÃºmero)", "1", input_type=int) - 1
        judge_name = self.available_judges[idx]
        resolution, default_k = self.detect_judge_params(judge_name)
        
        while True:
            print("\n" + "="*60)
            print("ğŸ“‹ ESTRATEGIAS DE EVALUACIÃ“N")
            print("="*60)
            print("1. ğŸ² Random Pixels - SelecciÃ³n aleatoria (baseline)")
            print("2. â­ Optimal Pixels - PÃ­xeles que maximizan confianza")
            print("3. ğŸ’€ Adversarial Pixels - PÃ­xeles que minimizan confianza")
            print("4. ğŸš« Adversarial Non-Zero - Adversariales SIN pÃ­xeles negros")
            print("5. ğŸ¤– Greedy Agent - SelecciÃ³n secuencial con agente Greedy")
            print("6. ğŸ§  MCTS Agent - SelecciÃ³n secuencial con agente MCTS")
            print("7. ğŸ’€ Greedy Adversarial Agent - Minimiza logits de clase verdadera")
            print("8. ğŸ§  MCTS Adversarial Agent - MCTS que maximiza predicciones incorrectas")
            print("9. ğŸ“Š Comparison Suite - Comparar todas las estrategias")
            print("10. ğŸ”¬ K-Range Analysis - Analizar diferentes valores de k")
            print("0. â†©ï¸  Volver al menÃº principal")
            
            choice = self.get_input("Selecciona estrategia", "9", 
                                  ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10"])
            
            if choice == "0":
                break
            elif choice == "1":
                self._evaluate_single_strategy(judge_name, resolution, default_k, "random")
            elif choice == "2":
                self._evaluate_single_strategy(judge_name, resolution, default_k, "optimal")
            elif choice == "3":
                self._evaluate_single_strategy(judge_name, resolution, default_k, "adversarial")
            elif choice == "4":
                self._evaluate_single_strategy(judge_name, resolution, default_k, "adversarial_nonzero")
            elif choice == "5":
                self._evaluate_single_strategy(judge_name, resolution, default_k, "greedy_agent")
            elif choice == "6":
                self._evaluate_single_strategy(judge_name, resolution, default_k, "mcts_agent")
            elif choice == "7":
                self._evaluate_single_strategy(judge_name, resolution, default_k, "greedy_adversarial_agent")
            elif choice == "8":
                self._evaluate_single_strategy(judge_name, resolution, default_k, "mcts_adversarial_agent")
            elif choice == "9":
                self._evaluate_comparison_suite(judge_name, resolution, default_k)
            elif choice == "10":
                self._evaluate_k_range_analysis(judge_name, resolution, default_k)

    def _evaluate_comparison_suite(self, judge_name, resolution, default_k):
        """Ejecuta comparaciÃ³n completa de estrategias."""
        print("\nğŸ“Š SUITE DE COMPARACIÃ“N COMPLETA")
        print("-" * 50)
        print("Compara todas las estrategias de evaluaciÃ³n del juez:")
        print("â€¢ EstÃ¡ticas: random, optimal, adversarial, adversarial_nonzero")
        print("â€¢ Agentes: greedy, MCTS, greedy adversarial, MCTS adversarial")
        
        k = self.get_input("NÃºmero de pÃ­xeles (k)", str(default_k), input_type=int)
        n_images = self.get_input("ImÃ¡genes por estrategia", "500", input_type=int)
        thr = self.get_input("Threshold", "0.0", input_type=float)
        
        include_agents = self.get_yes_no("Â¿Incluir estrategias de agentes (mÃ¡s lento)?", "y")
        allow_all_pixels = self.get_yes_no("Â¿Permitir pÃ­xeles negros para agentes?")
        save_comparison_outputs = self.get_yes_no("Â¿Guardar visualizaciones para comparaciÃ³n (una muestra por estrategia)?")
        
        strategies = ["random", "optimal", "adversarial", "adversarial_nonzero"]
        
        if include_agents:
            strategies.extend(["greedy_agent", "mcts_agent", "greedy_adversarial_agent", "mcts_adversarial_agent"])
            rollouts = self.get_input("Rollouts para MCTS agents", "300", input_type=int)
        
        print(f"\nğŸš€ Ejecutando comparaciÃ³n con k={k}, {n_images} imÃ¡genes cada una...")
        print(f"Estrategias: {', '.join(strategies)}")
        
        for strategy in strategies:
            cmd = (f"python eval_judge.py --judge_name {judge_name} --resolution {resolution} "
                   f"--strategy {strategy} --k {k} --n_images {n_images} --thr {thr}")
            
            if strategy in ["mcts_agent", "mcts_adversarial_agent"]:
                cmd += f" --rollouts {rollouts}"
                
            if strategy in ["greedy_agent", "mcts_agent", "greedy_adversarial_agent", "mcts_adversarial_agent"] and allow_all_pixels:
                cmd += " --allow_all_pixels"
            
            # Add saving options for comparison visualizations
            if save_comparison_outputs:
                # Save only a few samples for comparison purposes
                cmd += " --save_visualizations"
                # Limit to fewer images when saving outputs to avoid clutter
                if n_images > 50:
                    cmd = cmd.replace(f"--n_images {n_images}", "--n_images 10")
                    print(f"â„¹ï¸  Limitando a 10 imÃ¡genes para {strategy} (visualizaciÃ³n habilitada)")
            
            if self.run_command(cmd, f"ComparaciÃ³n - {strategy.replace('_', ' ').title()}"):
                print(f"âœ… {strategy.replace('_', ' ').title()} completado")
            else:
                print(f"âŒ Error en {strategy}")
                if not self.get_yes_no("Â¿Continuar con las demÃ¡s estrategias?"):
                    break
        
        print("\nğŸ“Š Suite de comparaciÃ³n completada.")
        print("ğŸ“ Resultados guardados en outputs/evaluations.csv")
        if save_comparison_outputs:
            print("ğŸ“ Visualizaciones guardadas en outputs/visualizations/evaluations/")
        print("ğŸ’¡ Use option 4 (Data Analysis) to export CSV data for analysis")

    def _evaluate_single_strategy(self, judge_name, resolution, default_k, strategy):
        """Ejecuta evaluaciÃ³n de una estrategia especÃ­fica."""
        print(f"\nğŸ¯ EVALUACIÃ“N - {strategy.replace('_', ' ').title()}")
        print("-" * 50)
        
        k = self.get_input("NÃºmero de pÃ­xeles (k)", str(default_k), input_type=int)
        n_images = self.get_input("NÃºmero de imÃ¡genes", "500", input_type=int)
        thr = self.get_input("Threshold", "0.0", input_type=float)
        
        cmd = (f"python eval_judge.py --judge_name {judge_name} --resolution {resolution} "
               f"--strategy {strategy} --k {k} --n_images {n_images} --thr {thr} "
               f"--seed {self.global_config['seed']}")
        
        # Opciones especÃ­ficas para agentes
        if strategy in ["mcts_agent", "mcts_adversarial_agent"]:
            rollouts = self.get_input("Rollouts para MCTS", "300", input_type=int)
            cmd += f" --rollouts {rollouts}"
        
        if strategy in ["greedy_agent", "mcts_agent", "greedy_adversarial_agent", "mcts_adversarial_agent"]:
            if self.get_yes_no("Â¿Permitir selecciÃ³n de pÃ­xeles negros?"):
                cmd += " --allow_all_pixels"
        
        # Ask about saving evaluation outputs
        print("\nğŸ“ Opciones de guardado para evaluaciÃ³n:")
        save_outputs = self.get_yes_no("Â¿Guardar imÃ¡genes y visualizaciones de la evaluaciÃ³n?")
        if save_outputs:
            cmd += " --save_metadata"  # This includes images, masks, and visualizations
        
        cmd += f' --note "single_eval_{strategy}"'
        
        if self.run_command(cmd, f"EvaluaciÃ³n {strategy.replace('_', ' ').title()}"):
            print(f"âœ… EvaluaciÃ³n {strategy} completada")
            if save_outputs:
                print(f"ğŸ“ Archivos guardados en outputs/visualizations/evaluations/")
            print("ğŸ“ Resultados guardados en outputs/evaluations.csv")
        else:
            print(f"âŒ Error en evaluaciÃ³n {strategy}")

    def _evaluate_k_range_analysis(self, judge_name, resolution, default_k):
        """Analiza diferentes valores de k para una estrategia."""
        print("\nğŸ”¬ ANÃLISIS DE RANGO K")
        print("-" * 50)
        print("EvalÃºa cÃ³mo cambia la precisiÃ³n del juez con diferentes valores de k")
        
        strategy = self.get_input("Estrategia a analizar", "random", 
                                ["random", "optimal", "adversarial", "adversarial_nonzero", 
                                 "greedy_agent", "mcts_agent", "greedy_adversarial_agent", "mcts_adversarial_agent"])
        
        print("Selecciona valores de k a probar:")
        k_values = []
        for k in [1, 2, 3, 4, 5, 6, 7, 8, 10, 12]:
            if self.get_yes_no(f"  â€¢ k = {k}"):
                k_values.append(k)
        
        if not k_values:
            return
            
        n_images = self.get_input("ImÃ¡genes por valor de k", "300", input_type=int)
        thr = self.get_input("Threshold", "0.0", input_type=float)
        
        if strategy in ["mcts_agent", "mcts_adversarial_agent"]:
            rollouts = self.get_input("Rollouts para MCTS", "200", input_type=int)
        
        allow_all_pixels = False
        if strategy in ["greedy_agent", "mcts_agent", "greedy_adversarial_agent", "mcts_adversarial_agent"]:
            allow_all_pixels = self.get_yes_no("Â¿Permitir pÃ­xeles negros?")
        
        save_k_analysis_outputs = self.get_yes_no("Â¿Guardar visualizaciones para anÃ¡lisis k (pocas muestras)?")
        
        print(f"\nğŸš€ Ejecutando anÃ¡lisis de k para {strategy}...")
        print(f"Valores de k: {k_values}")
        
        for k in k_values:
            cmd = (f"python eval_judge.py --judge_name {judge_name} --resolution {resolution} "
                   f"--strategy {strategy} --k {k} --n_images {n_images} --thr {thr} "
                   f"--seed {self.global_config['seed']}")
            
            if strategy in ["mcts_agent", "mcts_adversarial_agent"]:
                cmd += f" --rollouts {rollouts}"
            
            if allow_all_pixels:
                cmd += " --allow_all_pixels"
            
            # Add saving options for k analysis
            if save_k_analysis_outputs:
                cmd += " --save_visualizations"
                # Limit to very few images for k analysis
                if n_images > 20:
                    cmd = cmd.replace(f"--n_images {n_images}", "--n_images 5")
                    print(f"â„¹ï¸  Limitando a 5 imÃ¡genes para k={k} (visualizaciÃ³n habilitada)")
            
            cmd += f' --note "k_analysis_{strategy}_k{k}"'
            
            if self.run_command(cmd, f"K-Analysis {strategy} k={k}"):
                print(f"âœ… k={k} completado")
            else:
                print(f"âŒ Error con k={k}")
                if not self.get_yes_no("Â¿Continuar con los demÃ¡s valores?"):
                    break
        
        print("\nğŸ“Š AnÃ¡lisis de rango K completado")
        print("ğŸ“ Resultados guardados en outputs/evaluations.csv")

    def configure_experiments(self):
        """ConfiguraciÃ³n principal de experimentos."""
        print("\n" + "="*80)
        print("ğŸ”¬ CONFIGURACIÃ“N DE EXPERIMENTOS")
        print("="*80)
        print("Sistema inteligente de configuraciÃ³n con:")
        print("â€¢ ConfiguraciÃ³n global reutilizable")
        print("â€¢ 9 tipos de experimentos automatizados")
        print("â€¢ Opciones granulares de guardado")
        print("â€¢ GestiÃ³n de cola de experimentos")
        
        # Escanear jueces disponibles
        self.scan_available_judges()
        
        if not self.available_judges:
            print("âŒ No hay modelos juez disponibles. Entrena uno primero.")
            return
        
        # Configurar parÃ¡metros globales
        self._configure_global_parameters()
        
        # Configurar visualizaciones
        self._configure_visualizations()
        
        # Configurar tracking de logits
        self._configure_logits_tracking()
        
        # MenÃº de experimentos
        self._experiment_selection_menu()

    def _configure_global_parameters(self):
        """Configura parÃ¡metros globales que se reutilizan."""
        print("\nâš™ï¸ CONFIGURACIÃ“N GLOBAL")
        print("-" * 60)
        print("Estos parÃ¡metros se aplicarÃ¡n a todos los experimentos")
        
        # SelecciÃ³n de juez
        if self.global_config['judge_name'] is None or self.ask_override_global('judge_name', self.global_config['judge_name'], 'juez'):
            print("\nğŸ“Š Modelos juez disponibles:")
            for i, judge in enumerate(self.available_judges, 1):
                res, k = self.detect_judge_params(judge)
                print(f"{i:2d}. {judge} (res: {res}x{res}, k: {k})")
            
            idx = self.get_input("Selecciona juez (nÃºmero)", "1", input_type=int) - 1
            self.global_config['judge_name'] = self.available_judges[idx]
            detected_resolution, detected_k = self.detect_judge_params(self.global_config['judge_name'])
            self.global_config['resolution'] = detected_resolution
            
            # Preguntar sobre k
            if self.get_yes_no(f"Â¿Usar k={detected_k} detectado del juez?", "y"):
                self.global_config['k'] = detected_k
            else:
                self.global_config['k'] = self.get_input("NÃºmero de pÃ­xeles (k) global", "6", input_type=int)
        
        # Otros parÃ¡metros globales
        if self.ask_override_global('defaults', None, 'valores por defecto'):
            self.global_config['default_images_greedy'] = self.get_input(
                "ImÃ¡genes por defecto para Greedy", str(self.global_config['default_images_greedy']), input_type=int)
            self.global_config['default_images_mcts'] = self.get_input(
                "ImÃ¡genes por defecto para MCTS", str(self.global_config['default_images_mcts']), input_type=int)
            self.global_config['default_rollouts'] = self.get_input(
                "Rollouts por defecto para MCTS", str(self.global_config['default_rollouts']), input_type=int)
        
        if self.ask_override_global('advanced', None, 'opciones avanzadas'):
            self.global_config['allow_all_pixels'] = self.get_yes_no(
                "Â¿Permitir selecciÃ³n irrestricta de pÃ­xeles?")
            self.global_config['track_confidence'] = self.get_yes_no(
                "Â¿Trackear probabilidades del juez (para anÃ¡lisis estadÃ­stico)?")
        
        print("\nâœ… ConfiguraciÃ³n global establecida:")
        print(f"  â€¢ Juez: {self.global_config['judge_name']} (res: {self.global_config['resolution']}, k: {self.global_config['k']})")
        print(f"  â€¢ ImÃ¡genes defecto: Greedy={self.global_config['default_images_greedy']}, MCTS={self.global_config['default_images_mcts']}")
        print(f"  â€¢ Rollouts defecto: {self.global_config['default_rollouts']}")

    def _configure_visualizations(self):
        """Configura opciones granulares de guardado."""
        print("\nğŸ¨ CONFIGURACIÃ“N DE GUARDADO")
        print("-" * 50)
        print("Controla quÃ© se guarda durante los experimentos")
        
        self.config['viz_enabled'] = self.get_yes_no("Â¿Habilitar visualizaciones/guardado?")
        if self.config['viz_enabled']:
            print("\nğŸ“¦ MÃ©todo de configuraciÃ³n:")
            print("1. Presets rÃ¡pidos (recomendado)")
            print("2. SelecciÃ³n individual (control total)")
            method = self.get_input("Elige mÃ©todo", "1", options=["1", "2"])
            
            if method == "1":
                preset = self.get_input(
                    "Elige preset", "minimal", 
                    options=["minimal", "standard", "complete", "analysis", "json"])
                
                if preset == "minimal":
                    self.config.update({
                        'save_images': False, 'save_masks': False, 
                        'save_play': False, 'viz_colored': False, 
                        'save_confidence': False, 'save_json': False
                    })
                    print("âœ… Minimal: Solo resultados CSV (mÃ¡xima velocidad)")
                elif preset == "standard":
                    self.config.update({
                        'save_images': False, 'save_masks': False, 
                        'save_play': True, 'viz_colored': True, 
                        'save_confidence': False, 'save_json': False
                    })
                    print("âœ… Standard: Debates coloreados + secuencias")
                elif preset == "complete":
                    self.config.update({
                        'save_images': True, 'save_masks': True, 
                        'save_play': True, 'viz_colored': True, 
                        'save_confidence': True, 'save_json': False
                    })
                    print("âœ… Complete: Todas las visualizaciones y metadata")
                elif preset == "analysis":
                    self.config.update({
                        'save_images': False, 'save_masks': False, 
                        'save_play': True, 'viz_colored': False, 
                        'save_confidence': True, 'save_json': False
                    })
                    print("âœ… Analysis: Optimizado para anÃ¡lisis estadÃ­stico")
                elif preset == "json":
                    self.config.update({
                        'save_images': False, 'save_masks': False, 
                        'save_play': False, 'viz_colored': False, 
                        'save_confidence': False, 'save_json': True
                    })
                    print("âœ… JSON: Formato comprehensivo completo (reemplaza save_play)")
            else:
                # SelecciÃ³n individual
                print("\nğŸ“ Opciones individuales:")
                self.config['save_images'] = self.get_yes_no("  ğŸ–¼ï¸  Â¿Guardar imÃ¡genes originales?")
                self.config['save_masks'] = self.get_yes_no("  ğŸ­ Â¿Guardar imÃ¡genes enmascaradas?")
                self.config['save_play'] = self.get_yes_no("  ğŸ“œ Â¿Guardar secuencias de juego?")
                self.config['viz_colored'] = self.get_yes_no("  ğŸ¨ Â¿Guardar debates coloreados?")
                self.config['save_confidence'] = self.get_yes_no("  ğŸ“Š Â¿Guardar anÃ¡lisis de probabilidades (track_confidence)?")
                self.config['save_json'] = self.get_yes_no("  ğŸ“„ Â¿Guardar JSON comprehensivo?")
                
                # Mostrar advertencia si se seleccionan opciones redundantes
                if self.config['save_json'] and self.config['save_play']:
                    print("â„¹ï¸  NOTA: save_json incluye toda la funcionalidad de save_play")
                    if self.get_yes_no("  Â¿Desactivar save_play para evitar redundancia?"):
                        self.config['save_play'] = False
            
            # Metadata legacy para compatibilidad
            any_metadata = any([
                self.config['save_images'], self.config['save_masks'], 
                self.config['save_play'], self.config['save_confidence'],
                self.config['save_json']
            ])
            self.config['viz_metadata'] = any_metadata

    def _configure_logits_tracking(self):
        """Configura opciones de tracking de logits del juez."""
        print("\nğŸ§  CONFIGURACIÃ“N DE TRACKING DE LOGITS")
        print("-" * 50)
        print("Trackea la evoluciÃ³n de las predicciones del juez durante el debate")
        
        track_enabled = self.get_yes_no("Â¿Habilitar tracking de logits?", default="n")
        
        if track_enabled:
            print("\nğŸ“Š Opciones de tracking:")
            print("1. Solo estado final (bÃ¡sico)")
            print("2. Progresivo - cada pÃ­xel (recomendado)")
            print("3. Completo - progresivo + logits crudos")
            
            option = self.get_input("Elige opciÃ³n", "2", options=["1", "2", "3"])
            
            if option == "1":
                self.config['track_logits'] = True
                self.config['track_logits_progressive'] = False
                self.config['save_raw_logits'] = False
                print("âœ… Tracking bÃ¡sico: Solo estado final del juez")
            elif option == "2":
                self.config['track_logits'] = False
                self.config['track_logits_progressive'] = True
                self.config['save_raw_logits'] = False
                print("âœ… Tracking progresivo: Monitoreo despuÃ©s de cada pÃ­xel")
            elif option == "3":
                self.config['track_logits'] = False
                self.config['track_logits_progressive'] = True
                self.config['save_raw_logits'] = True
                print("âœ… Tracking completo: Progresivo + logits crudos para anÃ¡lisis")
                
            # Automatically enable JSON when logits tracking is used
            if not self.config.get('save_json', False):
                auto_json = self.get_yes_no("Â¿Habilitar JSON automÃ¡ticamente para guardar logits?", default=True)
                if auto_json:
                    self.config['save_json'] = True
                    print("ğŸ“ JSON habilitado automÃ¡ticamente")
        else:
            self.config['track_logits'] = False
            self.config['track_logits_progressive'] = False
            self.config['save_raw_logits'] = False
            print("âŒ Tracking de logits deshabilitado")

    def _get_advanced_flags(self):
        """Genera flags avanzados para comandos de debate."""
        flags = ""
        
        if self.global_config['allow_all_pixels']:
            flags += " --allow_all_pixels"
        
        if self.global_config['track_confidence'] or self.config['save_confidence']:
            flags += " --track_confidence"
        
        # Logits tracking options
        if self.config.get('track_logits'):
            flags += " --track_logits"
        
        if self.config.get('track_logits_progressive'):
            flags += " --track_logits_progressive"
        
        if self.config.get('save_raw_logits'):
            flags += " --save_raw_logits"
        
        # Comprehensive JSON format (replaces save_play, includes confidence_progression)
        if self.config['save_json']:
            flags += " --save_json"
        
        # Opciones granulares de guardado
        if self.config['viz_enabled']:
            if self.config['save_images']:
                flags += " --save_images"
            if self.config['save_masks']:
                flags += " --save_mask"
            if self.config['save_play']:
                flags += " --save_play"
            if self.config['viz_colored']:
                flags += " --save_colored_debate"
            
            # Flag metadata legacy
            if self.config['viz_metadata']:
                flags += " --save_metadata"
        
        return flags

    def _experiment_selection_menu(self):
        """MenÃº para selecciÃ³n de experimentos."""
        while True:
            print("\n" + "="*80)
            print("ğŸ“‹ SELECCIÃ“N DE EXPERIMENTOS")
            print("="*80)
            print(f"Juez: {self.global_config['judge_name']} | k: {self.global_config['k']} | "
                  f"Experimentos en cola: {len(self.experiments)}")
            print("-" * 80)
            
            print("1. ğŸ”„ Experimentos SimÃ©tricos (mismo tipo de agente)")
            print("2. âš”ï¸  Experimentos AsimÃ©tricos (MCTS vs Greedy)")
            print("3. ğŸ¯ Experimentos de AblaciÃ³n (variar k)")
            print("4. ğŸ“ˆ Curvas de Escalabilidad (rollouts)")
            print("5. ğŸ² AnÃ¡lisis de Robustez (mÃºltiples semillas)")
            print("6. ğŸ§  Experimentos de Confianza")
            print("7. ğŸš« Experimentos con PÃ­xeles Irrestrictos")
            print("8. ğŸ¤– Experimentos de EvaluaciÃ³n de Juez")
            print("9. ğŸ¯ Experimentos de Threshold Testing")
            print("10. ğŸ”¬ Experimentos Personalizados")
            print("11. ğŸ“Š Ver cola de experimentos")
            print("12. ğŸš€ Ejecutar experimentos")
            print("13. ğŸ’¾ Guardar configuraciÃ³n")
            print("0. â†©ï¸  Volver al menÃº principal")
            
            choice = self.get_input("Selecciona opciÃ³n", "12")
            
            if choice == "1":
                self._add_symmetric_experiments()
            elif choice == "2":
                self._add_asymmetric_experiments()
            elif choice == "3":
                self._add_ablation_experiments()
            elif choice == "4":
                self._add_scalability_experiments()
            elif choice == "5":
                self._add_robustness_experiments()
            elif choice == "6":
                self._add_confidence_experiments()
            elif choice == "7":
                self._add_unrestricted_pixel_experiments()
            elif choice == "8":
                self._add_judge_evaluation_experiments()
            elif choice == "9":
                self._add_threshold_testing_experiments()
            elif choice == "10":
                self._add_custom_experiments()
            elif choice == "11":
                self._show_experiment_queue()
            elif choice == "12":
                self._execute_experiments()
                break
            elif choice == "13":
                self._save_configuration()
            elif choice == "0":
                break

    def _add_symmetric_experiments(self):
        """AÃ±ade experimentos simÃ©tricos a la cola."""
        print("\nğŸ”„ EXPERIMENTOS SIMÃ‰TRICOS")
        print("-" * 40)
        print("Debates entre agentes del mismo tipo (Greedy vs Greedy, MCTS vs MCTS)")
        
        agent_type = self.get_input("Tipo de agente (greedy/mcts)", "greedy", ["greedy", "mcts"])
        
        variants = []
        if self.get_yes_no("â€¢ Baseline (liar primero, sin precommit)"):
            variants.append(("baseline", "", ""))
        if self.get_yes_no("â€¢ Con precommit"):
            variants.append(("precommit", " --precommit", "precommit"))
        if self.get_yes_no("â€¢ Honest primero"):
            variants.append(("honest_first", " --starts honest", "honest_first"))
        if self.get_yes_no("â€¢ Precommit + Honest primero"):
            variants.append(("precommit_honest", " --precommit --starts honest", "precommit_honest_first"))
        
        if not variants:
            return
        
        # ConfiguraciÃ³n especÃ­fica
        n_images = self.get_input("NÃºmero de imÃ¡genes", 
                                str(self.global_config[f'default_images_{agent_type}']), input_type=int)
        
        rollouts = ""
        if agent_type == "mcts":
            r = self.get_input("Rollouts", str(self.global_config['default_rollouts']), input_type=int)
            rollouts = f" --rollouts {r}"
            
        for variant_name, flags, note in variants:
            advanced_flags = self._get_advanced_flags()
            cmd = (f"python run_debate.py --judge_name {self.global_config['judge_name']} "
                   f"--resolution {self.global_config['resolution']} --k {self.global_config['k']} "
                   f"--thr {self.global_config['thr']} --agent_type {agent_type} "
                   f"--n_images {n_images} --seed {self.global_config['seed']}{rollouts}{flags}{advanced_flags} "
                   f'--note "{agent_type}_{note}"')
            
            desc = f"{agent_type.upper()} vs {agent_type.upper()} - {variant_name}"
            self.experiments.append((cmd, desc))
            
        print(f"âœ… AÃ±adidos {len(variants)} experimentos simÃ©tricos")

    def _add_asymmetric_experiments(self):
        """AÃ±ade experimentos asimÃ©tricos a la cola."""
        print("\nâš”ï¸ EXPERIMENTOS ASIMÃ‰TRICOS")
        print("-" * 40)
        print("Debates entre agentes de diferente tipo (MCTS vs Greedy)")
        
        honest_agent = self.get_input("Agente honesto (greedy/mcts)", "mcts", ["greedy", "mcts"])
        liar_agent = "greedy" if honest_agent == "mcts" else "mcts"
        
        variants = []
        if self.get_yes_no("â€¢ Baseline"):
            variants.append(("baseline", "", ""))
        if self.get_yes_no("â€¢ Con precommit"):
            variants.append(("precommit", " --precommit", "precommit"))
        
        if not variants:
            return
            
        n_images = self.get_input("NÃºmero de imÃ¡genes", 
                                str(self.global_config['default_images_mcts']), input_type=int)
        rollouts = self.get_input("Rollouts para MCTS", 
                                str(self.global_config['default_rollouts']), input_type=int)
        
        for variant_name, flags, note in variants:
            advanced_flags = self._get_advanced_flags()
            cmd = (f"python run_debate.py --judge_name {self.global_config['judge_name']} "
                   f"--resolution {self.global_config['resolution']} --k {self.global_config['k']} "
                   f"--thr {self.global_config['thr']} --mixed_agents --honest_agent {honest_agent} "
                   f"--rollouts {rollouts} --n_images {n_images} --seed {self.global_config['seed']}{flags}{advanced_flags} "
                   f'--note "{honest_agent}_honest_vs_{liar_agent}_liar_{note}"')
            
            desc = f"{honest_agent.upper()} Honest vs {liar_agent.upper()} Liar - {variant_name}"
            self.experiments.append((cmd, desc))
            
        print(f"âœ… AÃ±adidos {len(variants)} experimentos asimÃ©tricos")

    def _add_ablation_experiments(self):
        """AÃ±ade experimentos de ablaciÃ³n variando k."""
        print("\nğŸ¯ EXPERIMENTOS DE ABLACIÃ“N (VARIAR K)")
        print("-" * 40)
        print("Estudia el efecto del nÃºmero de pÃ­xeles revelados (k) en el rendimiento")
        
        k_values = []
        print("Selecciona valores de k a probar:")
        for k in [3, 4, 5, 6, 7, 8, 10, 12]:
            if self.get_yes_no(f"  â€¢ k = {k}"):
                k_values.append(k)
        
        if not k_values:
            return
            
        agent_type = self.get_input("Tipo de agente (greedy/mcts)", "greedy", ["greedy", "mcts"])
        n_images = self.get_input("ImÃ¡genes por valor de k", "500", input_type=int)
        
        if agent_type == "mcts":
            rollouts = self.get_input("Rollouts", "200", input_type=int)
            
        # Seleccionar variantes experimentales
        print("\nSelecciona variantes a incluir:")
        variants = []
        if self.get_yes_no("â€¢ Baseline (liar primero, sin precommit)"):
            variants.append(("baseline", "", ""))
        if self.get_yes_no("â€¢ Con precommit"):
            variants.append(("precommit", " --precommit", "precommit"))
        if self.get_yes_no("â€¢ Honest primero"):
            variants.append(("honest_first", " --starts honest", "honest_first"))
        if self.get_yes_no("â€¢ Precommit + Honest primero"):
            variants.append(("precommit_honest", " --precommit --starts honest", "precommit_honest_first"))
            
        # Experimentos asimÃ©tricos con granularidad completa
        asymmetric = self.get_yes_no("â€¢ Incluir experimentos asimÃ©tricos")
        asymmetric_combinations = []
        asymmetric_rollouts = {}
        
        if asymmetric:
            print("\nğŸ”§ CONFIGURACIÃ“N GRANULAR DE EXPERIMENTOS ASIMÃ‰TRICOS")
            print("-" * 50)
            print("Selecciona quÃ© combinaciones asimÃ©tricas incluir:")
            
            if self.get_yes_no("  â€¢ Greedy (honesto) vs MCTS (mentiroso)"):
                rollouts = self.get_input("    Rollouts para MCTS", "100", input_type=int)
                asymmetric_combinations.append(("greedy", "mcts"))
                asymmetric_rollouts[("greedy", "mcts")] = rollouts
                
            if self.get_yes_no("  â€¢ MCTS (honesto) vs Greedy (mentiroso)"):
                rollouts = self.get_input("    Rollouts para MCTS", "100", input_type=int)
                asymmetric_combinations.append(("mcts", "greedy"))
                asymmetric_rollouts[("mcts", "greedy")] = rollouts
            
            # Variantes especÃ­ficas para asimÃ©tricos
            print("\nSelecciona variantes especÃ­ficas para experimentos asimÃ©tricos:")
            asymmetric_variants = []
            if self.get_yes_no("  â€¢ Baseline asimÃ©trico (liar primero, sin precommit)"):
                asymmetric_variants.append(("baseline", "", ""))
            if self.get_yes_no("  â€¢ AsimÃ©trico con precommit"):
                asymmetric_variants.append(("precommit", " --precommit", "precommit"))
            if self.get_yes_no("  â€¢ AsimÃ©trico honest primero"):
                asymmetric_variants.append(("honest_first", " --starts honest", "honest_first"))
            if self.get_yes_no("  â€¢ AsimÃ©trico precommit + honest primero"):
                asymmetric_variants.append(("precommit_honest", " --precommit --starts honest", "precommit_honest_first"))
            
            if not asymmetric_variants:
                asymmetric_variants = [("baseline", "", "")]  # Default
                
            print(f"\nğŸ“Š ConfiguraciÃ³n asimÃ©trica:")
            print(f"    Combinaciones: {len(asymmetric_combinations)}")
            print(f"    Variantes: {len(asymmetric_variants)}")
            for combo in asymmetric_combinations:
                print(f"    â€¢ {combo[0]} vs {combo[1]} (rollouts: {asymmetric_rollouts[combo]})")
        else:
            asymmetric_variants = []
        
        if not variants:
            variants = [("baseline", "", "")]  # Default to baseline if none selected
        
        for k in k_values:
            advanced_flags = self._get_advanced_flags()
            
            # Experimentos simÃ©tricos
            for variant_name, variant_flags, variant_suffix in variants:
                cmd = (f"python run_debate.py --judge_name {self.global_config['judge_name']} "
                       f"--resolution {self.global_config['resolution']} --k {k} "
                       f"--thr {self.global_config['thr']} --n_images {n_images} "
                       f"--seed {self.global_config['seed']}")
                
                if agent_type == "mcts":
                    cmd += f" --agent_type mcts --rollouts {rollouts}"
                    desc = f"AblaciÃ³n k={k} - MCTS"
                else:
                    cmd += f" --agent_type greedy"
                    desc = f"AblaciÃ³n k={k} - Greedy"
                
                cmd += variant_flags
                variant_note = f"_{variant_suffix}" if variant_suffix else ""
                cmd += f'{advanced_flags} --note "ablation_k{k}_{agent_type}{variant_note}"'
                
                if variant_suffix:
                    desc += f" - {variant_name}"
                    
                self.experiments.append((cmd, desc))
            
            # Experimentos asimÃ©tricos con configuraciÃ³n granular
            if asymmetric and asymmetric_combinations:
                for honest_agent, liar_agent in asymmetric_combinations:
                    current_rollouts = asymmetric_rollouts[(honest_agent, liar_agent)]
                    
                    for variant_name, variant_flags, variant_suffix in asymmetric_variants:
                        cmd_asym = (f"python run_debate.py --judge_name {self.global_config['judge_name']} "
                                   f"--resolution {self.global_config['resolution']} --k {k} "
                                   f"--thr {self.global_config['thr']} --n_images {n_images} "
                                   f"--seed {self.global_config['seed']} --mixed_agents "
                                   f"--honest_agent {honest_agent} --rollouts {current_rollouts}")
                        cmd_asym += variant_flags
                        variant_note = f"_{variant_suffix}" if variant_suffix else ""
                        cmd_asym += f'{advanced_flags} --note "ablation_k{k}_asym_{honest_agent}_vs_{liar_agent}{variant_note}"'
                        
                        desc_asym = f"AblaciÃ³n k={k} - AsimÃ©trico ({honest_agent.title()} vs {liar_agent.title()})"
                        if variant_suffix:
                            desc_asym += f" - {variant_name}"
                        
                        self.experiments.append((cmd_asym, desc_asym))
            
        # Calcular total de experimentos con granularidad
        symmetric_total = len(k_values) * len(variants)
        asymmetric_total = 0
        if asymmetric and asymmetric_combinations:
            asymmetric_total = len(k_values) * len(asymmetric_combinations) * len(asymmetric_variants)
        
        total_experiments = symmetric_total + asymmetric_total
        print(f"âœ… AÃ±adidos {total_experiments} experimentos de ablaciÃ³n")
        print(f"    â€¢ SimÃ©tricos: {symmetric_total}")
        if asymmetric_total > 0:
            print(f"    â€¢ AsimÃ©tricos: {asymmetric_total} ({len(asymmetric_combinations)} combinaciones Ã— {len(asymmetric_variants)} variantes)")

    def _add_scalability_experiments(self):
        """AÃ±ade experimentos de escalabilidad de rollouts."""
        print("\nğŸ“ˆ CURVAS DE ESCALABILIDAD (ROLLOUTS)")
        print("-" * 40)
        print("Analiza cÃ³mo el rendimiento de MCTS escala con el nÃºmero de rollouts")
        
        print("Configurar serie de rollouts:")
        print("1. Serie logarÃ­tmica (50, 100, 200, 500, 1000, 2000)")
        print("2. Serie lineal (100, 200, 300, ..., 1000)")
        print("3. Serie personalizada")
        
        choice = self.get_input("Selecciona", "1", ["1", "2", "3"])
        
        if choice == "1":
            rollout_values = [50, 100, 200, 500, 1000, 2000]
        elif choice == "2":
            rollout_values = list(range(100, 1100, 100))
        else:
            rollout_values = []
            print("Ingresa valores de rollouts (escribe 'done' para terminar):")
            while True:
                val = input("Rollouts: ").strip()
                if val.lower() == 'done':
                    break
                try:
                    rollout_values.append(int(val))
                except:
                    print("âŒ Valor invÃ¡lido")
        
        # Filtrar segÃºn recursos computacionales
        if self.get_yes_no("Â¿Filtrar valores altos (>2000) por costo computacional?", "y"):
            original_len = len(rollout_values)
            rollout_values = [r for r in rollout_values if r <= 2000]
            if original_len > len(rollout_values):
                print(f"âš ï¸  Filtrados {original_len - len(rollout_values)} valores > 2000")
        
        n_images = self.get_input("ImÃ¡genes por punto", str(self.global_config['default_images_mcts']), input_type=int)
        fixed_seed = self.get_yes_no("Â¿Usar semilla fija para comparaciÃ³n?", "y")
        seed_to_use = self.global_config['seed'] if fixed_seed else None
        
        use_mixed = self.get_yes_no("Â¿Incluir agentes mixtos ademÃ¡s de MCTS puro?")
        
        for rollouts in rollout_values:
            # MCTS puro
            advanced_flags = self._get_advanced_flags()
            cmd = (f"python run_debate.py --judge_name {self.global_config['judge_name']} "
                   f"--resolution {self.global_config['resolution']} --k {self.global_config['k']} "
                   f"--thr {self.global_config['thr']} --agent_type mcts --rollouts {rollouts} "
                   f"--n_images {n_images}")
            
            if seed_to_use:
                cmd += f" --seed {seed_to_use}"
            else:
                cmd += f" --seed {self.global_config['seed']}"
                
            cmd += f'{advanced_flags} --note "scalability_mcts_r{rollouts}"'
            self.experiments.append((cmd, f"Escalabilidad MCTS - {rollouts} rollouts"))
            
            # Mixed si se solicita
            if use_mixed:
                cmd_mixed = cmd.replace("--agent_type mcts", "--mixed_agents --honest_agent mcts")
                cmd_mixed = cmd_mixed.replace("scalability_mcts", "scalability_mixed")
                self.experiments.append((cmd_mixed, f"Escalabilidad Mixed - {rollouts} rollouts"))
                
        print(f"âœ… AÃ±adidos {len(rollout_values) * (2 if use_mixed else 1)} experimentos")

    def _add_robustness_experiments(self):
        """AÃ±ade experimentos de robustez con mÃºltiples semillas."""
        print("\nğŸ² ANÃLISIS DE ROBUSTEZ (MÃšLTIPLES SEMILLAS)")
        print("-" * 40)
        print("EvalÃºa la consistencia de resultados con diferentes semillas aleatorias")
        
        seeds = []
        print("Configurar semillas:")
        print("1. Serie estÃ¡ndar (42, 123, 456, 789, 1337)")
        print("2. Serie personalizada")
        
        choice = self.get_input("Selecciona", "1", ["1", "2"])
        if choice == "1":
            seeds = [42, 123, 456, 789, 1337]
        else:
            n_seeds = self.get_input("NÃºmero de semillas", "5", input_type=int)
            for i in range(n_seeds):
                seed = self.get_input(f"Semilla {i+1}", str(42 + i*100), input_type=int)
                seeds.append(seed)
        
        # ConfiguraciÃ³n base
        agent_type = self.get_input("Tipo de agente (greedy/mcts/mixed)", "greedy", 
                                  ["greedy", "mcts", "mixed"])
        n_images = self.get_input("ImÃ¡genes por semilla", str(self.global_config['default_images_mcts']), input_type=int)
        
        base_cmd = (f"python run_debate.py --judge_name {self.global_config['judge_name']} "
                    f"--resolution {self.global_config['resolution']} --k {self.global_config['k']} "
                    f"--thr {self.global_config['thr']} --n_images {n_images}")
        
        if agent_type == "greedy":
            base_cmd += " --agent_type greedy"
        elif agent_type == "mcts":
            rollouts = self.get_input("Rollouts", str(self.global_config['default_rollouts']), input_type=int)
            base_cmd += f" --agent_type mcts --rollouts {rollouts}"
        else:  # mixed
            honest = self.get_input("Agente honesto", "mcts", ["greedy", "mcts"])
            rollouts = self.get_input("Rollouts", str(self.global_config['default_rollouts']), input_type=int)
            base_cmd += f" --mixed_agents --honest_agent {honest} --rollouts {rollouts}"
        
        for seed in seeds:
            advanced_flags = self._get_advanced_flags()
            cmd = base_cmd + f' --seed {seed}{advanced_flags} --note "robustness_{agent_type}_s{seed}"'
            self.experiments.append((cmd, f"Robustez {agent_type} - Semilla {seed}"))
            
        print(f"âœ… AÃ±adidos {len(seeds)} experimentos de robustez")

    def _add_confidence_experiments(self):
        """AÃ±ade experimentos especÃ­ficos para anÃ¡lisis de confianza."""
        print("\nğŸ§  EXPERIMENTOS DE CONFIANZA")
        print("-" * 40)
        print("Analiza la evoluciÃ³n de probabilidades del juez durante los debates")
        print("Estos experimentos fuerzan el tracking de probabilidades para anÃ¡lisis estadÃ­stico detallado.")
        
        # Temporalmente forzar track_confidence
        original_track = self.global_config['track_confidence']
        self.global_config['track_confidence'] = True
        
        agent_types = []
        if self.get_yes_no("â€¢ Incluir Greedy vs Greedy"):
            agent_types.append("greedy")
        if self.get_yes_no("â€¢ Incluir MCTS vs MCTS"):
            agent_types.append("mcts")
        if self.get_yes_no("â€¢ Incluir Mixed (MCTS vs Greedy)"):
            agent_types.append("mixed")
        
        if not agent_types:
            self.global_config['track_confidence'] = original_track
            return
        
        n_images = self.get_input("ImÃ¡genes por experimento", str(self.global_config['default_images_mcts']), input_type=int)
        k_values = []
        print("Valores de k para analizar confianza:")
        for k in [3, 4, 5, 6, 8]:
            if self.get_yes_no(f"  â€¢ k = {k}"):
                k_values.append(k)
        
        if not k_values:
            k_values = [self.global_config['k']]
        
        for agent_type in agent_types:
            for k in k_values:
                advanced_flags = self._get_advanced_flags()
                base_cmd = (f"python run_debate.py --judge_name {self.global_config['judge_name']} "
                           f"--resolution {self.global_config['resolution']} --k {k} "
                           f"--thr {self.global_config['thr']} --n_images {n_images} --seed {self.global_config['seed']}{advanced_flags}")
                
                if agent_type == "greedy":
                    cmd = base_cmd + f' --agent_type greedy --note "confidence_greedy_k{k}"'
                    desc = f"Confidence Analysis - Greedy k={k}"
                elif agent_type == "mcts":
                    rollouts = self.get_input(f"Rollouts para MCTS k={k}", str(self.global_config['default_rollouts']), input_type=int)
                    cmd = base_cmd + f' --agent_type mcts --rollouts {rollouts} --note "confidence_mcts_k{k}_r{rollouts}"'
                    desc = f"Confidence Analysis - MCTS k={k} ({rollouts}r)"
                else:  # mixed
                    honest = self.get_input("Agente honesto", "mcts", ["greedy", "mcts"])
                    rollouts = self.get_input(f"Rollouts para mixed k={k}", str(self.global_config['default_rollouts']), input_type=int)
                    cmd = base_cmd + f' --mixed_agents --honest_agent {honest} --rollouts {rollouts} --note "confidence_mixed_{honest}_k{k}"'
                    desc = f"Confidence Analysis - Mixed ({honest} honest) k={k}"
                
                self.experiments.append((cmd, desc))
        
        # Restaurar configuraciÃ³n original
        self.global_config['track_confidence'] = original_track
        
        total_experiments = len(agent_types) * len(k_values)
        print(f"âœ… AÃ±adidos {total_experiments} experimentos de anÃ¡lisis de confianza")

    def _add_unrestricted_pixel_experiments(self):
        """AÃ±ade experimentos con selecciÃ³n irrestricta de pÃ­xeles."""
        print("\nğŸš« EXPERIMENTOS CON PÃXELES IRRESTRICTOS")
        print("-" * 40)
        print("Permite a los agentes seleccionar CUALQUIER pÃ­xel, incluyendo pÃ­xeles negros,")
        print("para analizar estrategias emergentes y robustez del sistema.")
        
        # Temporalmente forzar allow_all_pixels
        original_allow = self.global_config['allow_all_pixels']
        self.global_config['allow_all_pixels'] = True
        
        experiment_types = []
        if self.get_yes_no("â€¢ ComparaciÃ³n: restringido vs irrestricto (mismo setup)"):
            experiment_types.append("comparison")
        if self.get_yes_no("â€¢ ExploraciÃ³n de estrategias adversariales"):
            experiment_types.append("adversarial")
        if self.get_yes_no("â€¢ AnÃ¡lisis de robustez con pÃ­xeles negros"):
            experiment_types.append("robustness")
        
        if not experiment_types:
            self.global_config['allow_all_pixels'] = original_allow
            return
        
        agent_type = self.get_input("Tipo de agente principal", "greedy", ["greedy", "mcts", "mixed"])
        n_images = self.get_input("ImÃ¡genes por experimento", str(self.global_config['default_images_mcts']), input_type=int)
        
        if "comparison" in experiment_types:
            # Experimentos de comparaciÃ³n directa
            advanced_flags = self._get_advanced_flags()
            base_cmd = (f"python run_debate.py --judge_name {self.global_config['judge_name']} "
                       f"--resolution {self.global_config['resolution']} --k {self.global_config['k']} "
                       f"--thr {self.global_config['thr']} --n_images {n_images} --seed {self.global_config['seed']}")
            
            # Con restricciÃ³n (temporalmente: quitar allow_all_pixels)
            self.global_config['allow_all_pixels'] = False
            restricted_flags = self._get_advanced_flags()
            self.global_config['allow_all_pixels'] = True
            
            if agent_type == "greedy":
                # Restringido
                cmd_restricted = base_cmd + restricted_flags + ' --agent_type greedy --note "comparison_restricted_greedy"'
                self.experiments.append((cmd_restricted, "Comparison - Greedy RESTRICTED pixels"))
                # Irrestricto
                cmd_unrestricted = base_cmd + advanced_flags + ' --agent_type greedy --note "comparison_unrestricted_greedy"'
                self.experiments.append((cmd_unrestricted, "Comparison - Greedy UNRESTRICTED pixels"))
        
        if "adversarial" in experiment_types:
            # Experimentos adversariales
            advanced_flags = self._get_advanced_flags()
            for k in [4, 6, 8]:
                cmd = (f"python run_debate.py --judge_name {self.global_config['judge_name']} "
                       f"--resolution {self.global_config['resolution']} --k {k} "
                       f"--thr 0.0 --n_images {n_images} --agent_type {agent_type} --seed {self.global_config['seed']}{advanced_flags} "
                       f'--note "adversarial_unrestricted_{agent_type}_k{k}"')
                self.experiments.append((cmd, f"Adversarial - {agent_type.upper()} k={k} (unrestricted)"))
        
        if "robustness" in experiment_types:
            # Experimentos de robustez
            advanced_flags = self._get_advanced_flags()
            for thr in [0.0, 0.1, 0.3]:
                cmd = (f"python run_debate.py --judge_name {self.global_config['judge_name']} "
                       f"--resolution {self.global_config['resolution']} --k {self.global_config['k']} "
                       f"--thr {thr} --n_images {n_images} --agent_type {agent_type} --seed {self.global_config['seed']}{advanced_flags} "
                       f'--note "robustness_unrestricted_{agent_type}_thr{thr}"')
                self.experiments.append((cmd, f"Robustness - {agent_type.upper()} thr={thr} (unrestricted)"))
        
        # Restaurar configuraciÃ³n original
        self.global_config['allow_all_pixels'] = original_allow
        
        print(f"âœ… AÃ±adidos experimentos de pÃ­xeles irrestrictos")
        print(f"    Los agentes pueden ahora seleccionar pÃ­xeles negros y explorar nuevas estrategias")

    def _add_judge_evaluation_experiments(self):
        """AÃ±ade experimentos especÃ­ficos de evaluaciÃ³n del juez con agentes."""
        print("\nğŸ¤– EXPERIMENTOS DE EVALUACIÃ“N DE JUEZ")
        print("-" * 40)
        print("EvalÃºa la capacidad del juez con diferentes estrategias de selecciÃ³n de pÃ­xeles")
        print("incluyendo agentes que seleccionan pÃ­xeles secuencialmente.")
        
        experiment_types = []
        if self.get_yes_no("â€¢ ComparaciÃ³n de todas las estrategias (8 estrategias)"):
            experiment_types.append("full_comparison")
        if self.get_yes_no("â€¢ AnÃ¡lisis de agentes vs estrategias estÃ¡ticas"):
            experiment_types.append("agent_vs_static")
        if self.get_yes_no("â€¢ AnÃ¡lisis de escalabilidad granular (estrategias y k personalizables)"):
            experiment_types.append("agent_scalability")
        if self.get_yes_no("â€¢ AnÃ¡lisis de rollouts para MCTS"):
            experiment_types.append("mcts_rollouts")
        
        if not experiment_types:
            return
        
        n_images = self.get_input("ImÃ¡genes por experimento", "300", input_type=int)
        
        if "full_comparison" in experiment_types:
            # ComparaciÃ³n completa de todas las estrategias
            strategies = ["random", "optimal", "adversarial", "adversarial_nonzero", 
                         "greedy_agent", "mcts_agent", "greedy_adversarial_agent", "mcts_adversarial_agent"]
            
            for strategy in strategies:
                cmd = (f"python eval_judge.py --judge_name {self.global_config['judge_name']} "
                       f"--resolution {self.global_config['resolution']} --k {self.global_config['k']} "
                       f"--thr {self.global_config['thr']} --strategy {strategy} --n_images {n_images} "
                       f"--seed {self.global_config['seed']}")
                
                if strategy in ["mcts_agent", "mcts_adversarial_agent"]:
                    cmd += " --rollouts 300"
                
                cmd += f' --note "judge_eval_comparison_{strategy}"'
                
                desc = f"Judge Eval - {strategy.replace('_', ' ').title()}"
                self.experiments.append((cmd, desc))
        
        if "agent_vs_static" in experiment_types:
            # ComparaciÃ³n directa agentes vs estÃ¡ticas con mismas condiciones
            for k in [4, 6, 8]:
                base_cmd = (f"python eval_judge.py --judge_name {self.global_config['judge_name']} "
                           f"--resolution {self.global_config['resolution']} --k {k} "
                           f"--thr {self.global_config['thr']} --n_images {n_images} --seed {self.global_config['seed']}")
                
                # EstÃ¡ticas
                for strategy in ["random", "optimal"]:
                    cmd = base_cmd + f' --strategy {strategy} --note "judge_eval_static_{strategy}_k{k}"'
                    self.experiments.append((cmd, f"Judge Eval - {strategy.title()} k={k}"))
                
                # Agentes
                for agent in ["greedy_agent", "mcts_agent", "greedy_adversarial_agent", "mcts_adversarial_agent"]:
                    cmd = base_cmd + f' --strategy {agent}'
                    if agent in ["mcts_agent", "mcts_adversarial_agent"]:
                        cmd += " --rollouts 200"
                    cmd += f' --note "judge_eval_agent_{agent}_k{k}"'
                    self.experiments.append((cmd, f"Judge Eval - {agent.replace('_', ' ').title()} k={k}"))
        
        if "agent_scalability" in experiment_types:
            # AnÃ¡lisis de escalabilidad granular con selecciÃ³n personalizada
            print("\nğŸ”§ CONFIGURACIÃ“N GRANULAR DE ESCALABILIDAD")
            print("-" * 50)
            
            # SelecciÃ³n granular de estrategias para escalabilidad
            all_strategies = ["random", "optimal", "adversarial", "adversarial_nonzero", 
                             "greedy_agent", "mcts_agent", "greedy_adversarial_agent", "mcts_adversarial_agent"]
            
            selected_scalability_strategies = []
            scalability_strategy_configs = {}
            
            print("Selecciona estrategias para anÃ¡lisis de escalabilidad:")
            for strategy in all_strategies:
                if self.get_yes_no(f"  â€¢ {strategy.replace('_', ' ').title()}"):
                    selected_scalability_strategies.append(strategy)
                    
                    # ConfiguraciÃ³n especÃ­fica para estrategias MCTS
                    if strategy in ["mcts_agent", "mcts_adversarial_agent"]:
                        rollouts = self.get_input(f"    Rollouts para {strategy}", "200", input_type=int)
                        scalability_strategy_configs[strategy] = {"rollouts": rollouts}
                    else:
                        scalability_strategy_configs[strategy] = {}
            
            if not selected_scalability_strategies:
                print("  âŒ No se seleccionaron estrategias para escalabilidad")
            else:
                # SelecciÃ³n granular de valores de k
                print("\nSelecciona valores de k para escalabilidad:")
                available_k_scalability = [3, 4, 5, 6, 7, 8, 10, 12, 15]
                selected_k_scalability = []
                
                for k in available_k_scalability:
                    if self.get_yes_no(f"  â€¢ k = {k}"):
                        selected_k_scalability.append(k)
                
                if not selected_k_scalability:
                    selected_k_scalability = [3, 4, 5, 6, 7, 8, 10]  # Default
                    print(f"  Usando valores de k por defecto: {selected_k_scalability}")
                
                # ConfiguraciÃ³n granular de thresholds para escalabilidad
                print("\nConfiguraciÃ³n de threshold para escalabilidad:")
                thr_mode = self.get_input("Usar threshold global o personalizado (global/personalizado)", 
                                         "global", ["global", "personalizado"])
                
                if thr_mode == "global":
                    scalability_thresholds = [self.global_config['thr']]
                else:
                    scalability_thresholds = []
                    threshold_options = [0.0, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0]
                    print("Selecciona thresholds para escalabilidad:")
                    for thr in threshold_options:
                        if self.get_yes_no(f"  â€¢ Threshold {thr}"):
                            scalability_thresholds.append(thr)
                    
                    if not scalability_thresholds:
                        scalability_thresholds = [self.global_config['thr']]  # Default
                
                # Generar experimentos de escalabilidad
                scalability_count = 0
                print(f"\nğŸ“Š Generando experimentos de escalabilidad:")
                print(f"    Estrategias: {len(selected_scalability_strategies)}")
                print(f"    Valores de k: {len(selected_k_scalability)}")
                print(f"    Thresholds: {len(scalability_thresholds)}")
                
                for strategy in selected_scalability_strategies:
                    for k in selected_k_scalability:
                        for thr in scalability_thresholds:
                            cmd = (f"python eval_judge.py --judge_name {self.global_config['judge_name']} "
                                   f"--resolution {self.global_config['resolution']} --k {k} "
                                   f"--thr {thr} --strategy {strategy} --n_images {n_images} "
                                   f"--seed {self.global_config['seed']}")
                            
                            # AÃ±adir configuraciÃ³n especÃ­fica de la estrategia
                            if strategy in scalability_strategy_configs and "rollouts" in scalability_strategy_configs[strategy]:
                                cmd += f" --rollouts {scalability_strategy_configs[strategy]['rollouts']}"
                            
                            cmd += f' --note "judge_eval_scalability_{strategy}_k{k}_thr{thr}"'
                            
                            desc = f"Judge Eval Scalability - {strategy.replace('_', ' ').title()} (k={k}, thr={thr})"
                            self.experiments.append((cmd, desc))
                            scalability_count += 1
                
                print(f"    Total experimentos de escalabilidad: {scalability_count}")
        
        if "mcts_rollouts" in experiment_types:
            # AnÃ¡lisis de rollouts para MCTS
            for rollouts in [50, 100, 200, 500, 1000]:
                cmd = (f"python eval_judge.py --judge_name {self.global_config['judge_name']} "
                       f"--resolution {self.global_config['resolution']} --k {self.global_config['k']} "
                       f"--thr {self.global_config['thr']} --strategy mcts_agent --rollouts {rollouts} "
                       f"--n_images {n_images} --seed {self.global_config['seed']} --note judge_eval_mcts_r{rollouts}")
                
                self.experiments.append((cmd, f"Judge Eval - MCTS {rollouts} rollouts"))
        
        total_added = len([exp for exp in self.experiments 
                          if "judge_eval" in exp[0]])
        print(f"âœ… AÃ±adidos experimentos de evaluaciÃ³n de juez")
        print(f"    Total: {total_added} experimentos que evalÃºan capacidades del juez")

    def _add_threshold_testing_experiments(self):
        """AÃ±ade experimentos de threshold testing con granularidad completa."""
        print("\nğŸ¯ EXPERIMENTOS DE THRESHOLD TESTING")
        print("-" * 40)
        print("Testa el juez con todas las estrategias disponibles")
        print("para valores de threshold personalizables")
        
        # SelecciÃ³n granular de estrategias
        print("\nğŸ”§ SELECCIÃ“N GRANULAR DE ESTRATEGIAS")
        print("-" * 40)
        available_strategies = ["random", "optimal", "adversarial", "adversarial_nonzero", 
                               "greedy_agent", "mcts_agent", "greedy_adversarial_agent", "mcts_adversarial_agent"]
        
        selected_strategies = []
        strategy_configs = {}
        
        for strategy in available_strategies:
            if self.get_yes_no(f"â€¢ {strategy.replace('_', ' ').title()}"):
                selected_strategies.append(strategy)
                
                # ConfiguraciÃ³n especÃ­fica para estrategias MCTS
                if strategy in ["mcts_agent", "mcts_adversarial_agent"]:
                    rollouts = self.get_input(f"  Rollouts para {strategy}", "200", input_type=int)
                    strategy_configs[strategy] = {"rollouts": rollouts}
                else:
                    strategy_configs[strategy] = {}
        
        if not selected_strategies:
            print("âŒ No se seleccionÃ³ ninguna estrategia")
            return
        
        # ConfiguraciÃ³n granular de thresholds
        print("\nğŸšï¸ CONFIGURACIÃ“N DE THRESHOLDS")
        print("-" * 40)
        threshold_mode = self.get_input("Modo de threshold (completo/personalizado/rango)", 
                                      "completo", ["completo", "personalizado", "rango"])
        
        if threshold_mode == "completo":
            # Thresholds completos de 0.0 a 1.0 con incrementos de 0.1
            thresholds = [round(i * 0.1, 1) for i in range(11)]
            print(f"  Usando thresholds completos: {thresholds}")
        
        elif threshold_mode == "personalizado":
            # SelecciÃ³n manual de thresholds especÃ­ficos
            all_thresholds = [round(i * 0.1, 1) for i in range(11)]
            thresholds = []
            print("Selecciona thresholds especÃ­ficos:")
            for thr in all_thresholds:
                if self.get_yes_no(f"  â€¢ Threshold {thr}"):
                    thresholds.append(thr)
            
            if not thresholds:
                thresholds = [0.0, 0.5, 1.0]  # Default
                print(f"  Usando thresholds por defecto: {thresholds}")
        
        elif threshold_mode == "rango":
            # Rango personalizado con incremento especÃ­fico
            start = self.get_input("Threshold inicial", "0.0", input_type=float)
            end = self.get_input("Threshold final", "1.0", input_type=float)
            step = self.get_input("Incremento", "0.1", input_type=float)
            
            thresholds = []
            current = start
            while current <= end:
                thresholds.append(round(current, 1))
                current += step
            print(f"  Usando rango personalizado: {thresholds}")
        
        # ConfiguraciÃ³n granular de parÃ¡metros
        print("\nâš™ï¸ CONFIGURACIÃ“N DE PARÃMETROS")
        print("-" * 40)
        
        # Valores de k personalizables
        k_mode = self.get_input("Usar k global o personalizado (global/personalizado)", 
                               "global", ["global", "personalizado"])
        
        if k_mode == "global":
            k_values = [self.global_config['k']]
        else:
            k_values = []
            available_k = [3, 4, 5, 6, 7, 8, 10, 12]
            print("Selecciona valores de k:")
            for k in available_k:
                if self.get_yes_no(f"  â€¢ k = {k}"):
                    k_values.append(k)
            
            if not k_values:
                k_values = [self.global_config['k']]  # Default
        
        n_images = self.get_input("ImÃ¡genes por experimento", "300", input_type=int)
        
        # Generar experimentos
        total_added = 0
        
        print(f"\nğŸ“Š GENERANDO EXPERIMENTOS...")
        print(f"    Estrategias: {len(selected_strategies)}")
        print(f"    Thresholds: {len(thresholds)}")
        print(f"    Valores de k: {len(k_values)}")
        
        for strategy in selected_strategies:
            print(f"\n  Configurando {strategy}...")
            
            for k in k_values:
                for thr in thresholds:
                    cmd = (f"python eval_judge.py --judge_name {self.global_config['judge_name']} "
                           f"--resolution {self.global_config['resolution']} --k {k} "
                           f"--thr {thr} --strategy {strategy} --n_images {n_images} "
                           f"--seed {self.global_config['seed']}")
                    
                    # AÃ±adir configuraciÃ³n especÃ­fica de la estrategia
                    if strategy in strategy_configs and "rollouts" in strategy_configs[strategy]:
                        cmd += f" --rollouts {strategy_configs[strategy]['rollouts']}"
                    
                    cmd += f' --note "threshold_test_{strategy}_k{k}_thr{thr}"'
                    
                    desc = f"Threshold Test - {strategy.replace('_', ' ').title()} (k={k}, thr={thr})"
                    self.experiments.append((cmd, desc))
                    total_added += 1
        
        print(f"âœ… AÃ±adidos {total_added} experimentos de threshold testing")
        print(f"    Estrategias: {', '.join([s.replace('_', ' ').title() for s in selected_strategies])}")
        print(f"    Thresholds: {len(thresholds)} valores ({min(thresholds)} - {max(thresholds)})")
        print(f"    Valores de k: {k_values}")
        print(f"    Total combinaciones: {len(selected_strategies)} Ã— {len(k_values)} Ã— {len(thresholds)} = {total_added}")

    def _add_custom_experiments(self):
        """AÃ±ade experimentos completamente personalizados."""
        print("\nğŸ”¬ EXPERIMENTO PERSONALIZADO")
        print("-" * 40)
        print("Configura un experimento con control total sobre todos los parÃ¡metros")
        
        # Construir comando paso a paso
        cmd = f"python run_debate.py --judge_name {self.global_config['judge_name']}"
        cmd += f" --resolution {self.global_config['resolution']}"
        
        # k personalizado
        k = self.get_input("PÃ­xeles (k)", str(self.global_config['k']), input_type=int)
        cmd += f" --k {k}"
        
        # threshold
        thr = self.get_input("Threshold", str(self.global_config['thr']), input_type=float)
        cmd += f" --thr {thr}"
        
        # Tipo de agente
        print("\nTipo de debate:")
        print("1. Greedy vs Greedy")
        print("2. MCTS vs MCTS")
        print("3. Mixed (diferentes tipos)")
        
        debate_type = self.get_input("Selecciona", "1", ["1", "2", "3"])
        
        if debate_type == "1":
            cmd += " --agent_type greedy"
            agent_desc = "Greedy"
        elif debate_type == "2":
            rollouts = self.get_input("Rollouts", "500", input_type=int)
            cmd += f" --agent_type mcts --rollouts {rollouts}"
            agent_desc = f"MCTS ({rollouts}r)"
        else:
            honest = self.get_input("Agente honesto (greedy/mcts)", "mcts", ["greedy", "mcts"])
            rollouts = self.get_input("Rollouts para MCTS", "500", input_type=int)
            cmd += f" --mixed_agents --honest_agent {honest} --rollouts {rollouts}"
            agent_desc = f"Mixed ({honest} honest)"
        
        # Otros parÃ¡metros
        n_images = self.get_input("NÃºmero de imÃ¡genes", "100", input_type=int)
        cmd += f" --n_images {n_images}"
        
        if self.get_yes_no("Â¿Usar precommit?"):
            cmd += " --precommit"
            
        if self.get_yes_no("Â¿Honest empieza primero?"):
            cmd += " --starts honest"
        
        # Opciones avanzadas
        if self.get_yes_no("Â¿Permitir selecciÃ³n irrestricta de pÃ­xeles?"):
            cmd += " --allow_all_pixels"
        
        if self.get_yes_no("Â¿Trackear confianza del juez?"):
            cmd += " --track_confidence"
        
        if self.get_yes_no("Â¿Guardar datos completos en formato JSON?"):
            cmd += " --save_json"
        
        cmd += f" --seed {self.global_config['seed']}"
            
        note = self.get_input("Nota descriptiva", f"custom_{agent_desc}")
        cmd += f' --note "{note}"'
        
        # Visualizaciones
        advanced_flags = self._get_advanced_flags()
        cmd += advanced_flags
        
        desc = self.get_input("DescripciÃ³n del experimento", f"Custom - {agent_desc}")
        
        self.experiments.append((cmd, desc))
        print("âœ… Experimento personalizado aÃ±adido")

    def _show_experiment_queue(self):
        """Muestra y gestiona la cola de experimentos."""
        if not self.experiments:
            print("\nâŒ No hay experimentos en la cola")
            return
            
        print("\n" + "="*80)
        print(f"ğŸ“Š COLA DE EXPERIMENTOS ({len(self.experiments)} total)")
        print("="*80)
        
        for i, (cmd, desc) in enumerate(self.experiments, 1):
            print(f"{i:3d}. {desc}")
        
        print("\nOpciones de gestiÃ³n:")
        print("1. ğŸ“ Ver comando completo de un experimento")
        print("2. âŒ Eliminar experimento especÃ­fico")
        print("3. ğŸ—‘ï¸ Limpiar toda la cola")
        print("4. â­ï¸ Reordenar experimentos")
        print("5. ğŸ“‹ Exportar lista de comandos")
        print("0. â†©ï¸ Volver")
        
        choice = self.get_input("Selecciona", "0")
        
        if choice == "1":
            idx = self.get_input("NÃºmero de experimento a ver", input_type=int) - 1
            if 0 <= idx < len(self.experiments):
                cmd, desc = self.experiments[idx]
                print(f"\nğŸ“‹ Experimento {idx+1}: {desc}")
                print(f"ğŸ”§ Comando: {cmd}")
        elif choice == "2":
            idx = self.get_input("NÃºmero a eliminar", input_type=int) - 1
            if 0 <= idx < len(self.experiments):
                removed = self.experiments.pop(idx)
                print(f"âœ… Eliminado: {removed[1]}")
        elif choice == "3":
            if self.get_yes_no("Â¿Seguro que quieres limpiar toda la cola?"):
                self.experiments = []
                print("âœ… Cola limpiada")
        elif choice == "4":
            self._reorder_experiments()
        elif choice == "5":
            self._export_commands()

    def _reorder_experiments(self):
        """Reordena experimentos en la cola."""
        if len(self.experiments) < 2:
            print("âŒ Necesitas al menos 2 experimentos para reordenar")
            return
        
        print("Opciones de reordenamiento:")
        print("1. Mover experimento a nueva posiciÃ³n")
        print("2. Intercambiar dos experimentos")
        print("3. Ordenar por tipo (simÃ©tricos primero)")
        
        choice = self.get_input("Selecciona", "1", ["1", "2", "3"])
        
        if choice == "1":
            idx1 = self.get_input("Mover experimento #", input_type=int) - 1
            idx2 = self.get_input("A posiciÃ³n #", input_type=int) - 1
            if 0 <= idx1 < len(self.experiments) and 0 <= idx2 < len(self.experiments):
                self.experiments.insert(idx2, self.experiments.pop(idx1))
                print("âœ… Experimento movido")
        elif choice == "2":
            idx1 = self.get_input("Primer experimento #", input_type=int) - 1
            idx2 = self.get_input("Segundo experimento #", input_type=int) - 1
            if 0 <= idx1 < len(self.experiments) and 0 <= idx2 < len(self.experiments):
                self.experiments[idx1], self.experiments[idx2] = self.experiments[idx2], self.experiments[idx1]
                print("âœ… Experimentos intercambiados")
        elif choice == "3":
            # Ordenar por tipo
            symmetric = [exp for exp in self.experiments if "vs" in exp[1] and exp[1].split("vs")[0].strip() == exp[1].split("vs")[1].strip().split("-")[0].strip()]
            asymmetric = [exp for exp in self.experiments if exp not in symmetric]
            self.experiments = symmetric + asymmetric
            print(f"âœ… Reordenados: {len(symmetric)} simÃ©tricos, {len(asymmetric)} otros")

    def _export_commands(self):
        """Exporta la lista de comandos a un archivo."""
        if not self.experiments:
            print("âŒ No hay experimentos para exportar")
            return
        
        filename = self.get_input("Nombre del archivo", "experiment_commands.txt")
        
        try:
            with open(filename, 'w') as f:
                f.write("# Lista de comandos de experimentos\n")
                f.write(f"# Generado: {datetime.now().isoformat()}\n")
                f.write(f"# Total de experimentos: {len(self.experiments)}\n\n")
                
                for i, (cmd, desc) in enumerate(self.experiments, 1):
                    f.write(f"# {i}. {desc}\n")
                    f.write(f"{cmd}\n\n")
            
            print(f"âœ… Comandos exportados a {filename}")
        except Exception as e:
            print(f"âŒ Error exportando: {e}")

    def _execute_experiments(self):
        """Ejecuta todos los experimentos en la cola."""
        if not self.experiments:
            print("\nâŒ No hay experimentos para ejecutar")
            return
            
        print("\n" + "="*80)
        print("ğŸš€ RESUMEN DE EJECUCIÃ“N")
        print("="*80)
        print(f"Total de experimentos: {len(self.experiments)}")
        print(f"ConfiguraciÃ³n global:")
        print(f"  â€¢ Juez: {self.global_config['judge_name']}")
        print(f"  â€¢ ResoluciÃ³n: {self.global_config['resolution']}")
        print(f"  â€¢ K: {self.global_config['k']}")
        print(f"  â€¢ Opciones de guardado: {'Habilitadas' if self.config['viz_enabled'] else 'Solo CSV'}")
        
        # Advertir sobre experimentos costosos
        costly_experiments = []
        for cmd, desc in self.experiments:
            if "--rollouts" in cmd:
                try:
                    rollouts = int(cmd.split("--rollouts")[1].split()[0])
                    if rollouts > 1000:
                        costly_experiments.append((desc, rollouts))
                except:
                    pass
        
        if costly_experiments:
            print("\nâš ï¸  ADVERTENCIA: Los siguientes experimentos son computacionalmente costosos:")
            for desc, rollouts in costly_experiments:
                print(f"   â€¢ {desc} ({rollouts} rollouts)")
        
        if not self.get_yes_no("\nÂ¿Proceder con la ejecuciÃ³n?"):
            return
            
        # EjecuciÃ³n
        print("\n" + "="*80)
        print("ğŸ”„ EJECUTANDO EXPERIMENTOS")
        print("="*80)
        
        successful = 0
        failed = 0
        start_time = datetime.now()
        
        for i, (cmd, desc) in enumerate(self.experiments, 1):
            print(f"\nğŸ“ Experimento {i}/{len(self.experiments)} ({i/len(self.experiments)*100:.0f}%)")
            
            if self.run_command(cmd, desc):
                successful += 1
            else:
                failed += 1
                if not self.get_yes_no("âŒ Error. Â¿Continuar con los siguientes?"):
                    break
        
        # Resumen final
        end_time = datetime.now()
        duration = end_time - start_time
        print("\n" + "="*80)
        print("ğŸ EJECUCIÃ“N COMPLETADA")
        print("="*80)
        print(f"âœ… Exitosos: {successful}")
        print(f"âŒ Fallidos: {failed}")
        print(f"â±ï¸ Tiempo total: {duration}")
        print(f"\nğŸ“ Resultados guardados en:")
        print(f"   â€¢ outputs/debates.csv - Debates simÃ©tricos")
        print(f"   â€¢ outputs/debates_asimetricos.csv - Debates asimÃ©tricos")
        print(f"   â€¢ outputs/evaluations.csv - Evaluaciones de juez")
        if self.config['viz_enabled']:
            print(f"   â€¢ outputs/visualizations/ - ImÃ¡genes y metadata")
        print(f"\nğŸ’¡ Use option 4 (Data Analysis) to export experiment data")

    def run_command(self, cmd, description):
        """Ejecuta un comando y muestra el progreso."""
        print(f"\n{'='*60}")
        print(f"ğŸš€ {description}")
        print(f"ğŸ“ {cmd}")
        print(f"{'='*60}")
        
        try:
            result = subprocess.run(cmd, shell=True, check=True)
            print(f"âœ… Completado exitosamente")
            return True
        except subprocess.CalledProcessError as e:
            print(f"âŒ Error: {e}")
            return False

    def show_previous_results(self):
        """Muestra resumen de resultados anteriores."""
        print("\n" + "="*80)
        print("ğŸ“Š RESULTADOS ANTERIORES")
        print("="*80)
        print("Resumen de experimentos ejecutados y datos generados")
        
        # Verificar archivos CSV
        files_to_check = [
            ("outputs/debates.csv", "Debates simÃ©tricos"),
            ("outputs/debates_asimetricos.csv", "Debates asimÃ©tricos"),
            ("outputs/evaluations.csv", "Evaluaciones de juez"),
            ("outputs/judges.csv", "Jueces entrenados")
        ]
        
        total_records = 0
        for filepath, desc in files_to_check:
            if os.path.exists(filepath):
                print(f"\nğŸ“„ {desc} ({filepath}):")
                try:
                    with open(filepath, 'r') as f:
                        lines = f.readlines()
                        if len(lines) > 1:
                            records = len(lines)-1
                            total_records += records
                            print(f"   ğŸ“Š Total de registros: {records}")
                            print("   ğŸ“ˆ Ãšltimos resultados:")
                            for line in lines[-3:]:
                                if line.strip() and not line.startswith("timestamp"):
                                    parts = line.strip().split(',')
                                    if len(parts) > 2:
                                        if "judges.csv" in filepath:
                                            if len(parts) > 10:
                                                print(f"   - {parts[1]}: acc={parts[10]}")
                                        else:
                                            if len(parts) > 2:
                                                print(f"   - {parts[1]}: acc={parts[-2]}")
                except Exception as e:
                    print(f"   âŒ Error leyendo archivo: {e}")
            else:
                print(f"\nâŒ No encontrado: {filepath}")
        
        print(f"\nğŸ“ˆ RESUMEN TOTAL:")
        print(f"   â€¢ Total de registros en todos los archivos: {total_records}")
        
        # Check data exports
        if os.path.exists("analysis/output"):
            csv_files = [f for f in os.listdir("outputs") if f.endswith('.csv')]
            print(f"   â€¢ CSV data files available: {len(csv_files)}")
        
        print(f"\nğŸ’¡ Consejos:")
        print(f"   â€¢ Use option 4 to access data analysis tools")
        print(f"   â€¢ Los archivos CSV pueden abrirse con Excel o Python/pandas")
        print(f"   â€¢ CSV files ready for custom analysis and visualization")

    def _save_configuration(self):
        """Guarda la configuraciÃ³n actual."""
        print("\nğŸ’¾ GUARDAR CONFIGURACIÃ“N")
        print("Guarda la configuraciÃ³n completa para reutilizaciÃ³n futura")
        
        filename = self.get_input("Nombre del archivo", "config_experimentos")
        if not filename.endswith('.json'):
            filename += '.json'
            
        config_data = {
            "global_config": self.global_config,
            "config": self.config,
            "experiments": self.experiments,
            "timestamp": datetime.now().isoformat(),
            "version": "3.3",
            "description": f"ConfiguraciÃ³n con {len(self.experiments)} experimentos"
        }
        
        filepath = get_config_path(filename)
        
        try:
            with open(filepath, 'w') as f:
                json.dump(config_data, f, indent=2)
                
            print(f"âœ… ConfiguraciÃ³n guardada en {filepath}")
            print(f"ğŸ“‹ Incluye:")
            print(f"   â€¢ ConfiguraciÃ³n global: {self.global_config['judge_name']}, k={self.global_config['k']}")
            print(f"   â€¢ ConfiguraciÃ³n de guardado: {'Habilitada' if self.config['viz_enabled'] else 'Solo CSV'}")
            print(f"   â€¢ Cola de experimentos: {len(self.experiments)} experimentos")
        except Exception as e:
            print(f"âŒ Error guardando configuraciÃ³n: {e}")

    def load_configuration(self):
        """Carga una configuraciÃ³n guardada."""
        print("\nğŸ“‚ CARGAR CONFIGURACIÃ“N")
        print("Carga una configuraciÃ³n previamente guardada")
        
        from utils.paths import CONFIGS_DIR
        if not CONFIGS_DIR.exists() or not any(CONFIGS_DIR.glob('*.json')):
            print("âŒ No hay configuraciones guardadas")
            print("ğŸ’¡ Usa opciÃ³n 12 en el menÃº de experimentos para guardar configuraciones")
            return
            
        configs = [f.name for f in CONFIGS_DIR.glob('*.json')]
        if not configs:
            print("âŒ No hay configuraciones guardadas")
            return
            
        print("ğŸ“ Configuraciones disponibles:")
        for i, config in enumerate(configs, 1):
            print(f"{i}. {config}")
            
        idx = self.get_input("Selecciona configuraciÃ³n", "1", input_type=int) - 1
        if 0 <= idx < len(configs):
            filepath = get_config_path(configs[idx])
            try:
                with open(filepath, 'r') as f:
                    data = json.load(f)
                    
                if "global_config" in data:
                    self.global_config = data["global_config"]
                self.config = data["config"]
                self.experiments = data["experiments"]
                
                print(f"âœ… ConfiguraciÃ³n cargada de {data.get('timestamp', 'fecha desconocida')}")
                print(f"ğŸ“‹ ConfiguraciÃ³n cargada:")
                print(f"   â€¢ Juez: {self.global_config['judge_name']}")
                print(f"   â€¢ ResoluciÃ³n: {self.global_config['resolution']}")
                print(f"   â€¢ K: {self.global_config['k']}")
                print(f"   â€¢ Experimentos: {len(self.experiments)}")
                print(f"   â€¢ Guardado: {'Habilitado' if self.config['viz_enabled'] else 'Solo CSV'}")
                
            except Exception as e:
                print(f"âŒ Error cargando configuraciÃ³n: {e}")

    def create_template(self):
        """Crea templates predefinidos."""
        print("\nğŸ“ CREAR TEMPLATE DE EXPERIMENTOS")
        print("-" * 40)
        print("Templates optimizados para diferentes propÃ³sitos de investigaciÃ³n")
        
        print("\nğŸ“š Templates disponibles:")
        print("1. ğŸ“š Paper Replication - ConfiguraciÃ³n del paper original")
        print("2. ğŸš€ Quick Test - Prueba rÃ¡pida con pocos recursos")
        print("3. ğŸ”¬ Full Analysis - AnÃ¡lisis exhaustivo")
        print("4. ğŸ“Š Benchmarking - ComparaciÃ³n de agentes")
        print("5. ğŸ¯ Ablation Study - Estudio de ablaciÃ³n completo")
        
        choice = self.get_input("Selecciona template", "1", ["1", "2", "3", "4", "5"])
        
        # Limpiar experimentos actuales
        self.experiments = []
        
        # Configurar segÃºn template
        if choice == "1":
            self._create_paper_template()
        elif choice == "2":
            self._create_quick_template()
        elif choice == "3":
            self._create_full_template()
        elif choice == "4":
            self._create_benchmark_template()
        elif choice == "5":
            self._create_ablation_template()
            
        print(f"\nâœ… Template creado con {len(self.experiments)} experimentos")
        print("ğŸ“‹ Experimentos aÃ±adidos a la cola:")
        for i, (_, desc) in enumerate(self.experiments, 1):
            print(f"   {i}. {desc}")
            
        if self.get_yes_no("Â¿Guardar este template?"):
            self._save_configuration()

    def _create_paper_template(self):
        """Template basado en el paper original."""
        print("\nğŸ“š Configurando Paper Replication Template...")
        print("Replica los experimentos principales del paper AI Safety via Debate")
        
        # Configurar juez por defecto
        if "28" in self.available_judges:
            self.global_config['judge_name'] = "28"
            self.global_config['resolution'] = 28
            self.global_config['k'] = 6
        
        base_cmd = (f"python run_debate.py --judge_name {self.global_config['judge_name']} "
                    f"--resolution {self.global_config['resolution']} --k 6 --thr 0.0 "
                    f"--seed {self.global_config['seed']}")
        
        # Experimentos del paper
        experiments = [
            (f'{base_cmd} --agent_type greedy --n_images 1000 --note "paper_greedy_baseline"',
             "Paper - Greedy baseline"),
            (f'{base_cmd} --agent_type greedy --n_images 1000 --precommit --note "paper_greedy_precommit"',
             "Paper - Greedy precommit"),
            (f'{base_cmd} --agent_type mcts --rollouts 100 --n_images 500 --note "paper_mcts_100"',
             "Paper - MCTS 100 rollouts"),
            (f'{base_cmd} --agent_type mcts --rollouts 500 --n_images 500 --note "paper_mcts_500"',
             "Paper - MCTS 500 rollouts"),
            (f'{base_cmd} --agent_type mcts --rollouts 1000 --n_images 300 --note "paper_mcts_1000"',
             "Paper - MCTS 1000 rollouts"),
        ]
        
        self.experiments.extend(experiments)

    def _create_quick_template(self):
        """Template para pruebas rÃ¡pidas."""
        print("\nğŸš€ Configurando Quick Test Template...")
        print("Pruebas rÃ¡pidas para verificar funcionalidad del sistema")
        
        base_cmd = (f"python run_debate.py --judge_name {self.global_config['judge_name']} "
                    f"--resolution {self.global_config['resolution']} --k {self.global_config['k']} "
                    f"--thr {self.global_config['thr']} --seed {self.global_config['seed']}")
        
        experiments = [
            (f"{base_cmd} --agent_type greedy --n_images 100 --note quick_greedy",
             "Quick - Greedy test"),
            (f"{base_cmd} --agent_type mcts --rollouts 50 --n_images 50 --note quick_mcts",
             "Quick - MCTS test"),
            (f"{base_cmd} --mixed_agents --honest_agent mcts --rollouts 50 --n_images 30 --note quick_mixed",
             "Quick - Mixed test"),
        ]
        
        self.experiments.extend(experiments)

    def _create_full_template(self):
        """Template para anÃ¡lisis exhaustivo."""
        print("\nğŸ”¬ Configurando Full Analysis Template...")
        print("AnÃ¡lisis exhaustivo con todas las combinaciones posibles")
        
        base_cmd = (f"python run_debate.py --judge_name {self.global_config['judge_name']} "
                    f"--resolution {self.global_config['resolution']} --k {self.global_config['k']} "
                    f"--thr {self.global_config['thr']} --seed {self.global_config['seed']}")
        
        # Combinaciones completas
        for agent in ["greedy", "mcts"]:
            for precommit in ["", " --precommit"]:
                for starts in ["liar", "honest"]:
                    n_img = 1000 if agent == "greedy" else 500
                    rollouts = " --rollouts 500" if agent == "mcts" else ""
                    
                    note = f"full_{agent}"
                    if precommit:
                        note += "_precommit"
                    note += f"_{starts}first"
                    
                    cmd = (f"{base_cmd} --agent_type {agent}{rollouts} --n_images {n_img} "
                           f"--starts {starts}{precommit} --note \"{note}\"")
                    
                    desc = f"Full - {agent.upper()}"
                    if precommit:
                        desc += " + precommit"
                    desc += f" ({starts} first)"
                    
                    self.experiments.append((cmd, desc))

    def _create_benchmark_template(self):
        """Template para benchmarking."""
        print("\nğŸ“Š Configurando Benchmarking Template...")
        print("ComparaciÃ³n directa de agentes con condiciones controladas")
        
        base_cmd = (f"python run_debate.py --judge_name {self.global_config['judge_name']} "
                    f"--resolution {self.global_config['resolution']} --k {self.global_config['k']} "
                    f"--thr {self.global_config['thr']} --seed {self.global_config['seed']}")
        
        experiments = [
            (f"{base_cmd} --agent_type greedy --n_images 500 --note bench_greedy",
             "Benchmark - Greedy vs Greedy"),
            (f"{base_cmd} --agent_type mcts --rollouts 100 --n_images 300 --note bench_mcts_100",
             "Benchmark - MCTS 100r"),
            (f"{base_cmd} --agent_type mcts --rollouts 500 --n_images 300 --note bench_mcts_500",
             "Benchmark - MCTS 500r"),
            (f"{base_cmd} --agent_type mcts --rollouts 1000 --n_images 200 --note bench_mcts_1000",
             "Benchmark - MCTS 1000r"),
            (f"{base_cmd} --mixed_agents --honest_agent mcts --rollouts 500 --n_images 300 --note bench_mixed_mcts_h",
             "Benchmark - MCTS honest vs Greedy liar"),
            (f"{base_cmd} --mixed_agents --honest_agent greedy --rollouts 500 --n_images 300 --note bench_mixed_greedy_h",
             "Benchmark - Greedy honest vs MCTS liar"),
        ]
        
        self.experiments.extend(experiments)

    def _create_ablation_template(self):
        """Template para estudio de ablaciÃ³n."""
        print("\nğŸ¯ Configurando Ablation Study Template...")
        print("Estudio sistemÃ¡tico de parÃ¡metros k y rollouts")
        
        # AblaciÃ³n de k
        for k in [3, 4, 5, 6, 8, 10]:
            cmd = (f"python run_debate.py --judge_name {self.global_config['judge_name']} "
                   f"--resolution {self.global_config['resolution']} --k {k} "
                   f"--thr {self.global_config['thr']} --agent_type greedy --n_images 500 "
                   f"--seed {self.global_config['seed']} --note ablation_k{k}")
            self.experiments.append((cmd, f"Ablation - k={k}"))
        
        # AblaciÃ³n de rollouts
        for r in [50, 100, 200, 500, 1000, 2000]:
            cmd = (f"python run_debate.py --judge_name {self.global_config['judge_name']} "
                   f"--resolution {self.global_config['resolution']} --k {self.global_config['k']} "
                   f"--thr {self.global_config['thr']} --agent_type mcts --rollouts {r} "
                   f"--n_images 200 --seed {self.global_config['seed']} --note ablation_r{r}")
            self.experiments.append((cmd, f"Ablation - {r} rollouts"))

def main():
    """Punto de entrada principal."""
    manager = ExperimentManager()
    manager.print_banner()
    manager.scan_available_judges()
    manager.show_main_menu()

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâŒ EjecuciÃ³n interrumpida por el usuario")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nğŸ’¥ Error inesperado: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)